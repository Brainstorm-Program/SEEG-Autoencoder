{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Sub-Module of the EEG-GAN Package\n",
    "\n",
    "The autoencdoer code we are using today is part of our larger package, [EEG-GAN](https://autoresearch.github.io/EEG-GAN/). This package uses generative modelling - specifically, generative adversarial networks - to increase sample sizes by creating synthetic participants. We have shown that doing this enhances classification performance by providing more training samples for classification models. You can read more about this in our [published manuscript](https://escholarship.org/uc/item/9gz8g908). This package may actually be useful here with this data for this challenge; however, it has not been tested using SEEG data nor with so many conditions (i.e., up to 30 video clips) and with so little samples, so we opted not to present this package here. \n",
    "\n",
    "With that said, we are actively developing EEG-GAN v2.0, and this future version will contain an embedded autoencoder, which is the code included within this repo. Although this package is in active development, the autoencoder component is stabilized and thoroughly tested to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as sst\n",
    "\n",
    "import torch\n",
    "from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present\n",
    "from nn_architecture.ae_networks import TransformerAutoencoder, TransformerDoubleAutoencoder, TransformerFlattenAutoencoder\n",
    "from helpers.dataloader import Dataloader\n",
    "\n",
    "pd.set_option('display.max_columns', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_data(samples, fs=1024, notch_freq=60.0, quality_factor=20.0, data_prefix='Time'):\n",
    "    \n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    b_notch, a_notch = ss.iirnotch(notch_freq, quality_factor, fs)\n",
    "    filtered_signal = ss.filtfilt(b_notch, a_notch, samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    samples.iloc[:, neural_indices[0]:] = filtered_signal\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def filter_data(samples, lowcut=0.1, highcut=200, fs=1024, order=5, padding=3, data_prefix='Time'):\n",
    "    '''\n",
    "    ...add here\n",
    "    '''\n",
    "    padding_datapoints = padding*fs\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = ss.butter(order, [low, high], btype='bandpass')\n",
    "    \n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Filter signal\n",
    "    filtered_signal = ss.filtfilt(b, a, samples.iloc[:, neural_indices[0]:], padlen=(padding*fs), axis=1)\n",
    "    samples.iloc[:, neural_indices[0]:] = filtered_signal\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def artifact_rejection(samples, threshold=5, data_prefix='Time'):\n",
    "\n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Determine gradients and z-scores\n",
    "    grad = np.gradient(samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    g_zs = sst.zscore(grad, axis=1)\n",
    "    h_zs = sst.zscore(samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    \n",
    "    #Determine which segments should be removed\n",
    "    reject = np.logical_or(np.abs(g_zs) > threshold, np.abs(h_zs) > threshold) #Determine all datapoints that violate criteria\n",
    "    reject = np.any(reject, axis=1)\n",
    "    \n",
    "    #Convert to which segments should be kept\n",
    "    keep = [not r for r in reject]\n",
    "    \n",
    "    #Report\n",
    "    print(f\"{np.sum(reject)} of {len(samples)} ({np.round((np.sum(reject)/len(samples))*100)}%) of segments were rejected.\")\n",
    "    \n",
    "    return samples.loc[keep]\n",
    "\n",
    "def downsample_data(samples, sampling_rate=1024, downsampled_rate=256, data_prefix='Time'):\n",
    "\n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Determine length of samples in seconds\n",
    "    number_of_seconds = samples.iloc[0][neural_indices[0]:].shape[0]/sampling_rate\n",
    "    \n",
    "    #Determine number of datapoints after downsampling\n",
    "    new_number_of_datapoints = int(number_of_seconds*downsampled_rate)\n",
    "    \n",
    "    #Determine new column names\n",
    "    sr_ratio = sampling_rate/downsampled_rate\n",
    "    metadata_names = samples.columns[:neural_indices[0]]\n",
    "    downsampled_time_names = [column_name for column_index, column_name in enumerate(samples.columns[neural_indices[0]:]) if column_index%sr_ratio == 0]\n",
    "    downsampled_names = list(metadata_names) + list(downsampled_time_names)\n",
    "    \n",
    "    #Iterate through samples to downsample\n",
    "    downsampled_samples = pd.DataFrame(columns=downsampled_names)\n",
    "    for sample_index in range(len(samples)):\n",
    "        current_sample = samples.iloc[sample_index]\n",
    "        current_metadata = current_sample.iloc[:neural_indices[0]]\n",
    "        current_sample = current_sample.iloc[neural_indices[0]:]\n",
    "        downsampled_sample = scipy.signal.resample(current_sample, new_number_of_datapoints)\n",
    "        new_sample = pd.DataFrame([list(current_metadata) + list(downsampled_sample)], columns=downsampled_names)\n",
    "        downsampled_samples = pd.concat([downsampled_samples, new_sample])\n",
    "\n",
    "    return downsampled_samples       \n",
    "\n",
    "def initiate_autoencoder(ae_dict, dataset):\n",
    "\n",
    "    n_channels = dataset.shape[-1]\n",
    "    sequence_length = dataset.shape[1] - dataloader.labels.shape[1]\n",
    "\n",
    "    if ae_dict['configuration']['target'] == 'channels':\n",
    "        autoencoder = TransformerAutoencoder(input_dim=n_channels,\n",
    "                                       output_dim=ae_dict['configuration']['channels_out'],\n",
    "                                       output_dim_2=sequence_length,\n",
    "                                       target=TransformerAutoencoder.TARGET_CHANNELS,\n",
    "                                       hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                       num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                       num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    elif ae_dict['configuration']['target'] == 'time':\n",
    "        autoencoder = TransformerAutoencoder(input_dim=sequence_length,\n",
    "                                       output_dim=ae_dict['configuration']['timeseries_out'],\n",
    "                                       output_dim_2=n_channels,\n",
    "                                       target=TransformerAutoencoder.TARGET_TIMESERIES,\n",
    "                                       hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                       num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                       num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    elif ae_dict['configuration']['target'] == 'full':\n",
    "        autoencoder = TransformerDoubleAutoencoder(input_dim=n_channels,\n",
    "                                             output_dim=ae_dict['configuration']['output_dim'],\n",
    "                                             output_dim_2=ae_dict['configuration']['output_dim_2'],\n",
    "                                             sequence_length=sequence_length,\n",
    "                                             hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                             num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                             num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    else:\n",
    "        raise ValueError(f\"Encode target '{ae_dict['configuration']['target']}' not recognized, options are 'channels', 'time', or 'full'.\")\n",
    "    consume_prefix_in_state_dict_if_present(ae_dict['model'],'module.')\n",
    "    autoencoder.load_state_dict(ae_dict['model'])\n",
    "    autoencoder.device = torch.device('cpu')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Re-Organizing the Data\n",
    "\n",
    "The autoencoder needs a very specific data structure to work. The csv files provided for each participant is already in this data stucture, so we will not have to do too much data manipulation in this workshop, which is great.\n",
    "\n",
    "We use a very simple 2-dimensional data structure where each row is a different trial-level sample for one condition and one electrode. We will direct the autoencoder with column names to guide it in understanding what is metadata and what is neural data. For example, below we will inform the autoencoder that the electrode labels are within the `Electrode` column (i.e., `channel_label=Electrode`) and that the neural data begins with the prefix `Time` (i.e., `kw_timestep=Time`). Behind the scenes, the autoencoder will use the electrode labels to transform the data from a 2D matrix to a 3D matrix and will only include data with the given prefix as the neural data. The autoencoder will ignore any other columns not specified. For example, when looking at the data snippet below, it will ignore the columns `ParticipantID`, `Phase`, and `Condition` because it does not need this information.\n",
    "\n",
    "| participant_id | Phase | Condition | Electrode | Time0000 | Time0001 | Time0002 | Time0003 | ... |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| e0010GP | Encoding | 1 | HIP1 | 2.28 | -4.44 | -0.98 | -5.67 | ... |\n",
    "| e0010GP | Encoding | 1 | HIP2 | 11.67 | 0.66 | 1.43 | 11.62 | ... |\n",
    "| e0010GP | Encoding | 2 | HIP1 | 11.90 | 8.67 | 1.85 | 0.73 | ... |\n",
    "| e0010GP | Encoding | 2 | HIP2 | 6.73 | 3.63 | 3.80 | 5.63 | ... |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "In this workshop, we will be looking through participant data multiple times. Instead of re-loading each dataset over and over again, we might as well load them all once and keep them stored in a variable. We will do this by creating a dictionary where each key is a different participant's dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:29<00:00, 22.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Keys: dict_keys(['e0010GP', 'e0011XQ', 'e0013LW', 'e0020JA'])\n",
      "\n",
      "Example dataset:\n",
      "     Participant_ID          Phase  ...    Time5118   Time5119\n",
      "0           e0010GP       Encoding  ...   75.100557  36.021683\n",
      "1           e0010GP       Encoding  ...    1.462135  -1.462135\n",
      "2           e0010GP       Encoding  ...  -42.401907 -51.706401\n",
      "3           e0010GP       Encoding  ...  101.684825  95.038758\n",
      "4           e0010GP       Encoding  ...  -64.466850 -65.264378\n",
      "...             ...            ...  ...         ...        ...\n",
      "7585        e0010GP  NextDayRecall  ...    0.664607 -17.944381\n",
      "7586        e0010GP  NextDayRecall  ...   42.933593  39.211795\n",
      "7587        e0010GP  NextDayRecall  ...  -36.021683 -11.298314\n",
      "7588        e0010GP  NextDayRecall  ...   19.273594   5.715618\n",
      "7589        e0010GP  NextDayRecall  ...   32.565728  21.134493\n",
      "\n",
      "[7590 rows x 5126 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#First, we designate where the data is\n",
    "data_path = os.path.abspath('/oscar/data/brainstorm-ws/seeg_data/Memory Task Data/Epilepsy/Monitoring/')\n",
    "\n",
    "#We will also manually set which participants to include in the autoencoder training\n",
    "participant_files = [\n",
    "              f'{data_path}/e0010GP_data.csv',\n",
    "              f'{data_path}/e0011XQ_data.csv',\n",
    "              f'{data_path}/e0013LW_data.csv',\n",
    "              #f'{data_path}/e0015TJ_data.csv', #TODO: ADD THIS BACK, JUST PROCESSING NOW\n",
    "              #f'{data_path}/e0017MC_data.csv', #Does not have any electrodes with the HIP naming convention\n",
    "              #f'{data_path}/e0019VQ_data.csv', #TODO: ADD THIS BACK, JUST PROCESSING NOW\n",
    "              f'{data_path}/e0020JA_data.csv',\n",
    "              #f'{data_path}/e0024DV_data.csv' #TODO: ADD THIS BACK, JUST PROCESSING NOW\n",
    "              ]\n",
    "\n",
    "#We will determine all participant ids \n",
    "participant_ids = [participant_file.split('/')[-1].replace('_data.csv','') for participant_file in participant_files]\n",
    "\n",
    "#Now, let's load all participant data and save them into a dictionary\n",
    "participant_data = {} #Create empty dictionary\n",
    "for participant_index, participant_file in enumerate(tqdm(participant_files)):\n",
    "    participant_data[participant_ids[participant_index]] = pd.read_csv(participant_file, dtype = {'Electrode': str})\n",
    "                                                                       \n",
    "#We can report what we created\n",
    "print(f\"Dictionary Keys: {participant_data.keys()}\\n\")\n",
    "print('Example dataset:')\n",
    "print(participant_data[list(participant_data.keys())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrodes\n",
    "One other requirement that the autoencoder has is that every participant has the same number of electrodes. This toolbox was designed for EEG analyses, and this is the standard practice in that research---but as we have seen throughout this week, our participants have both a different number of electrodes and electrodes in different locations. What this means is that we are going to have to make some decisions on how many electrodes, and which electrodes, to include for each participant.\n",
    "\n",
    "Later on, we are going to use an adapted version of Younes Strittmatter's classification notebook, which focuses on hippocampus electrodes (i.e., uses electrodes with the `HIP` acronym), so here we will only focus on these electrodes. Let's cycle through each participant and see which hippocampus electrodes they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant: e0010GP\n",
      "Electrodes (24):\n",
      "['PHIP1', 'PHIP10', 'PHIP11', 'PHIP12', 'PHIP2', 'PHIP3', 'PHIP4', 'PHIP5', 'PHIP6', 'PHIP7', 'PHIP8', 'PHIP9', 'MHIP1', 'MHIP10', 'MHIP11', 'MHIP12', 'MHIP2', 'MHIP3', 'MHIP4', 'MHIP5', 'MHIP6', 'MHIP7', 'MHIP8', 'MHIP9']\n",
      "\n",
      "Participant: e0011XQ\n",
      "Electrodes (24):\n",
      "['HIPB1', 'HIPB10', 'HIPB2', 'HIPB3', 'HIPB4', 'HIPB5', 'HIPB6', 'HIPB7', 'HIPB8', 'HIPB9', 'HIPH1', 'HIPH10', 'HIPH11', 'HIPH12', 'HIPH2', 'HIPH3', 'HIPH4', 'HIPH5', 'HIPH6', 'HIPH7', 'HIPH8', 'HIPH9', 'HIPB11', 'HIPH13']\n",
      "\n",
      "Participant: e0013LW\n",
      "Electrodes (13):\n",
      "['Ahip1', 'Ahip10', 'Ahip11', 'Ahip12', 'Ahip13', 'Ahip2', 'Ahip3', 'Ahip4', 'Ahip5', 'Ahip6', 'Ahip7', 'Ahip8', 'Ahip9']\n",
      "\n",
      "Participant: e0020JA\n",
      "Electrodes (176):\n",
      "['LHIPBD1', 'LHIPBD10', 'LHIPBD11', 'LHIPBD12', 'LHIPBD13', 'LHIPBD14', 'LHIPBD2', 'LHIPBD3', 'LHIPBD4', 'LHIPBD5', 'LHIPBD6', 'LHIPBD7', 'LHIPBD8', 'LHIPBD9', 'LHIPHD1', 'LHIPHD10', 'LHIPHD11', 'LHIPHD12', 'LHIPHD13', 'LHIPHD14', 'LHIPHD2', 'LHIPHD3', 'LHIPHD4', 'LHIPHD5', 'LHIPHD6', 'LHIPHD7', 'LHIPHD8', 'LHIPHD9', 'LPHIPP1', 'LPHIPP10', 'LPHIPP11', 'LPHIPP12', 'LPHIPP13', 'LPHIPP14', 'LPHIPP15', 'LPHIPP2', 'LPHIPP3', 'LPHIPP4', 'LPHIPP5', 'LPHIPP6', 'LPHIPP7', 'LPHIPP8', 'LPHIPP9', 'RHIPBD1', 'RHIPBD10', 'RHIPBD11', 'RHIPBD12', 'RHIPBD13', 'RHIPBD14', 'RHIPBD15', 'RHIPBD2', 'RHIPBD3', 'RHIPBD4', 'RHIPBD5', 'RHIPBD6', 'RHIPBD7', 'RHIPBD8', 'RHIPBD9', 'RHIPHD1', 'RHIPHD10', 'RHIPHD11', 'RHIPHD12', 'RHIPHD13', 'RHIPHD14', 'RHIPHD15', 'RHIPHD2', 'RHIPHD3', 'RHIPHD4', 'RHIPHD5', 'RHIPHD6', 'RHIPHD7', 'RHIPHD8', 'RHIPHD9', 'RPHIPP1', 'RPHIPP10', 'RPHIPP11', 'RPHIPP12', 'RPHIPP13', 'RPHIPP14', 'RPHIPP15', 'RPHIPP2', 'RPHIPP3', 'RPHIPP4', 'RPHIPP5', 'RPHIPP6', 'RPHIPP7', 'RPHIPP8', 'RPHIPP9', 'LHIPBD1', 'LHIPBD10', 'LHIPBD11', 'LHIPBD12', 'LHIPBD13', 'LHIPBD14', 'LHIPBD2', 'LHIPBD3', 'LHIPBD4', 'LHIPBD5', 'LHIPBD6', 'LHIPBD7', 'LHIPBD8', 'LHIPBD9', 'LHIPHD1', 'LHIPHD10', 'LHIPHD11', 'LHIPHD12', 'LHIPHD13', 'LHIPHD14', 'LHIPHD2', 'LHIPHD3', 'LHIPHD4', 'LHIPHD5', 'LHIPHD6', 'LHIPHD7', 'LHIPHD8', 'LHIPHD9', 'LPHIPP1', 'LPHIPP10', 'LPHIPP11', 'LPHIPP12', 'LPHIPP13', 'LPHIPP14', 'LPHIPP15', 'LPHIPP2', 'LPHIPP3', 'LPHIPP4', 'LPHIPP5', 'LPHIPP6', 'LPHIPP7', 'LPHIPP8', 'LPHIPP9', 'RHIPBD1', 'RHIPBD10', 'RHIPBD11', 'RHIPBD12', 'RHIPBD13', 'RHIPBD14', 'RHIPBD15', 'RHIPBD2', 'RHIPBD3', 'RHIPBD4', 'RHIPBD5', 'RHIPBD6', 'RHIPBD7', 'RHIPBD8', 'RHIPBD9', 'RHIPHD1', 'RHIPHD10', 'RHIPHD11', 'RHIPHD12', 'RHIPHD13', 'RHIPHD14', 'RHIPHD15', 'RHIPHD2', 'RHIPHD3', 'RHIPHD4', 'RHIPHD5', 'RHIPHD6', 'RHIPHD7', 'RHIPHD8', 'RHIPHD9', 'RPHIPP1', 'RPHIPP10', 'RPHIPP11', 'RPHIPP12', 'RPHIPP13', 'RPHIPP14', 'RPHIPP15', 'RPHIPP2', 'RPHIPP3', 'RPHIPP4', 'RPHIPP5', 'RPHIPP6', 'RPHIPP7', 'RPHIPP8', 'RPHIPP9']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Determine the brain region of interest\n",
    "brain_region = 'HIP'\n",
    "\n",
    "#Iterate through all participants\n",
    "for participant_id in participant_data.keys():\n",
    "    print(f\"Participant: {participant_id}\") #Report which participant\n",
    "    electrodes = [electrode.split('_')[-1] for electrode in participant_data[participant_id]['Electrode'].unique() if brain_region.lower() in electrode.lower()] #Determine corresponding electrodes\n",
    "    print(f\"Electrodes ({len(electrodes)}):\\n{electrodes}\\n\") #Report found electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the participants have quite different electrodes within the hippocamps. This could mean that participants have electrodes in different locations but in some cases it could also simply indicate that there are different naming conventions per participant. Ideally, we would be able to find a number of electrodes that are completely consistent across participants but this may not be possible with the current dataset due to the nature of collecting SEEG data. \n",
    "\n",
    "So, instead of selecting common electrodes, we will randomly select a set of electrodes for each participant and hope that they are similar enough to consider consistent across participants. The classification code that Younes presented has us averaging across these electrodes anyhow, so this seems like a safe enough process here. \n",
    "\n",
    "The participants also have a different number of electrodes within the hippocampus, ranging from 13 to 176. As the autoencoder needs the same number of electrodes for each participant, we need to select a number of electrodes equal to or less than the lower end of this range. In other words, we'll need to select up to 13 electrodes per participant. I personally prefer even numbers, so let's choose 12 electrodes per participant. \n",
    "\n",
    "You could select which electrodes to include manually if you would like, but instead here we will randomly select 12 of the hippocampus electrodes per participant. We will set a seed in our `numpy` random function first so that we end up with the same 12 electrodes per participant every time we run this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   e0010GP e0011XQ e0013LW              e0020JA\n",
      "0    PHIP6   HIPB5   Ahip1   14_e0020JA_LHIPHD6\n",
      "1    MHIP2   HIPH6   Ahip2   14_e0020JA_LHIPBD2\n",
      "2    PHIP1  HIPB10  Ahip11  15_e0020JA_RHIPHD15\n",
      "3    MHIP4   HIPB7   Ahip3   14_e0020JA_RHIPBD9\n",
      "4    PHIP9   HIPB1  Ahip12  15_e0020JA_LHIPHD13\n",
      "5    PHIP7   HIPH1   Ahip4   15_e0020JA_RPHIPP8\n",
      "6   MHIP10  HIPH11  Ahip13   14_e0020JA_LHIPHD2\n",
      "7   PHIP10   HIPH7   Ahip7  15_e0020JA_RHIPHD13\n",
      "8    MHIP7   HIPB4   Ahip9   14_e0020JA_LHIPBD5\n",
      "9    PHIP3   HIPH8   Ahip6   15_e0020JA_LHIPHD7\n",
      "10  PHIP11   HIPB8  Ahip10  14_e0020JA_RHIPBD11\n",
      "11   MHIP1   HIPB3   Ahip8   14_e0020JA_LHIPHD3\n"
     ]
    }
   ],
   "source": [
    "#Set numpy seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Determine the brain region of interest\n",
    "brain_region = 'HIP'\n",
    "number_of_electrodes = 12\n",
    "\n",
    "#Iterate through each participant and collect 12 electrodes per participant \n",
    "selected_electrodes = {} #Setup a variable to collect the selected electrodes\n",
    "for participant_id in participant_data.keys():\n",
    "    electrodes = [electrode for electrode in participant_data[participant_id]['Electrode'].unique() if brain_region.lower() in electrode.lower()] #Determine corresponding electrodes\n",
    "    electrodes = np.random.choice(electrodes, number_of_electrodes, replace=False) #Randomly select a number of electrodes equal to the variable number_of_electrodes\n",
    "    selected_electrodes[participant_id] = electrodes #Save selected electrodes as a list of lists to use later\n",
    "    \n",
    "#Print as dataframe\n",
    "pd.set_option('display.max_columns', len(participant_files)) #Show more columns just for a moment\n",
    "print(pd.DataFrame(np.array([selected_electrodes[key] for key in selected_electrodes.keys()]).T, columns = [participant_file.split('/')[-1].replace('_data.csv','') for participant_file in participant_files]))\n",
    "pd.set_option('display.max_columns', 5) #Reset to showing 5 columns max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data Files\n",
    "\n",
    "Another requirement the autoencoder has is that all of the data is within the same csv file. Above we have created a list of participants to include and we have determined which electrodes to use per participant, so now we will extract the participant data for each participant and add it all to a single dataframe and then save this dataframe in our current repo's `data` sub-directory. \n",
    "\n",
    "The autoencoder should only be trained with the data that you will use to train your classification model. In the classification code, we will train our model on the `Encoding` and `SameDayRecall` phases and test it's performance on the `NextDayRecall` phase. As such, we will only include the `Encoding` and `SameDayRecall` phases in our new datafile.\n",
    "\n",
    "Furthermore, the autoencoder model can get quite big with large datasets, and having 5 seconds at 1024Hz (i.e., 5120 datapoints) is quite large. Large data and models may fail if they fill your memory. So, we will here condense the data in two ways. First, we are going to trim the data using the `cut_data` function below. In this workshop, we will cut the data to 3 seconds by removing the last 2 seconds. Second, we will downsample the data using the `downsample_data` function below. In this workshop we will reduce the sampling rate from 1024Hz to 256Hz. Together this means our sample will be reduced from 5120 datapoints to 768 datapoints, or in other words we will reduce the dataset size by 85%.\n",
    "\n",
    "\n",
    "WARNING: Sharp wave ripples (SWR) are in the 110 - 200Hz range and the frequencies you can extract from data equal half of the sampling rate (this is called the Nyquist Limit). So, if we downsample to 256Hz, we can only extract frequencies up to 128Hz, cutting out the majority of these SWR frequencies. If we want to keep these SWR frequencies, it would be better to downsample to 512Hz, which would allow us to extract frequencies up to 256Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 540 (0.0%) of segments were rejected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:06<00:20,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1080 (0.0%) of segments were rejected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:23<00:25, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 2160 (0.0%) of segments were rejected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:59<00:23, 23.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 420 (0.0%) of segments were rejected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:13<00:00, 18.30s/it]\n"
     ]
    }
   ],
   "source": [
    "#We will first create a dataframe to store all of our participant data in\n",
    "times = [f\"Time{str(datapoint).zfill(4)}\" for datapoint in range(5120)]\n",
    "ae_data = pd.DataFrame(columns=['Participant_ID','Phase','Condition','Electrode','Error_Position','Error_Color']+times)\n",
    "\n",
    "#We will indicate which phases we are going to include in the training file\n",
    "training_phases = ['Encoding', 'SameDayRecall']\n",
    "\n",
    "#Iterate through each participant, filtering the data to the specified phases and electrodes, and store it into the master variable\n",
    "for participant_id in tqdm(participant_data.keys()):\n",
    "    phase_indices = participant_data[participant_id]['Phase'].isin(training_phases) #Phase indices\n",
    "    electrode_indices = participant_data[participant_id]['Electrode'].isin(selected_electrodes[participant_id]) #Electrode indices\n",
    "    p_data = participant_data[participant_id][(phase_indices) & (electrode_indices)] #Extract participant data\n",
    "\n",
    "    #We also need to rename all of the electrodes so that there is consistency of electrode labels across participants\n",
    "    for electrode_index, selected_electrode in enumerate(selected_electrodes[participant_id]):\n",
    "        p_data = p_data.replace({selected_electrode: electrode_index+1})\n",
    "        \n",
    "    #Pre-Process Data\n",
    "    p_data = notch_data(p_data, fs=1024, notch_freq=60.0, quality_factor=20.0, data_prefix='Time')\n",
    "    p_data = filter_data(p_data, lowcut=0.1, highcut=120, fs=1024, order=5, padding=3, data_prefix='Time') #Note: You should really only filter the continuous data, and not segmented data like we are doing here because the latter will introduct 'edge artifacts'\n",
    "    p_data = artifact_rejection(p_data, threshold=5, data_prefix='Time')\n",
    "    p_data = downsample_data(p_data, sampling_rate=1024, downsampled_rate=256, data_prefix='Time')\n",
    "\n",
    "    #Add data to the master variable\n",
    "    ae_data = pd.concat([ae_data, p_data])\n",
    "    \n",
    "#Save data to csv\n",
    "ae_data.to_csv('data/seeg_ae_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the file is now within our directory. As this file is now ready and in our directory, we are ready to train the autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ganTrialElectrodeERP_p50_e8_len100.csv\tseeg_ae_training_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "The autoencoder is designed to be used within the command line interface (CLI). What this means is that we do not use functions, such as `train_autoencoder(training_data)` as is common with Python. Insteead we are going to run commands such as `python autoencoder_training_main.py ...` where `...` are our input parameters. If you are not familiar with CLI programming, not to worry as we will show you how to do so here. \n",
    "\n",
    "In jupyter notebooks, if you begin a line of code with `!` it will run via the CLI. Our Autoencoder has a `help` parameter so let's see how that works here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "-----------------------------------------\r\n",
      "Command line arguments:\r\n",
      "-----------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "INPUT HELP - These are the inputs that can be given from the command line\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "Input           | Type            | Description                                                                                   | Default value               \r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "ddp             | <class 'bool'>  | Activate distributed training                                                                 | False                       \r\n",
      "load_checkpoint | <class 'bool'>  | Load a pre-trained AE                                                                         | False                       \r\n",
      "ddp_backend     | <class 'str'>   | Backend for the DDP-Training; \"nccl\" for GPU; \"gloo\" for CPU;                                 | nccl                        \r\n",
      "path_dataset    | <class 'str'>   | Path to the dataset                                                                           | data/gansEEGTrainingData.csv\r\n",
      "path_checkpoint | <class 'str'>   | Path to a trained model to continue training                                                  | trained_ae/checkpoint.pt    \r\n",
      "save_name       | <class 'str'>   | Name to save model                                                                            | None                        \r\n",
      "target          | <class 'str'>   | Target dimension (channel, time, full) to encode; full is recommended for multi-channel data; | full                        \r\n",
      "channel_label   | <class 'str'>   | Column name to detect used channels                                                           |                             \r\n",
      "kw_timestep     | <class 'str'>   | Keyword for the time step of the dataset                                                      | Time                        \r\n",
      "activation      | <class 'str'>   | Activation function of the AE components; Options: [relu, leakyrelu, sigmoid, tanh, linear]   | sigmoid                     \r\n",
      "channels_out    | <class 'int'>   | Size of the encoded channels                                                                  | 10                          \r\n",
      "timeseries_out  | <class 'int'>   | Size of the encoded timeseries                                                                | 10                          \r\n",
      "n_epochs        | <class 'int'>   | Number of epochs to train for                                                                 | 100                         \r\n",
      "batch_size      | <class 'int'>   | Batch size                                                                                    | 128                         \r\n",
      "sample_interval | <class 'int'>   | Interval of epochs between saving samples                                                     | 100                         \r\n",
      "hidden_dim      | <class 'int'>   | Hidden dimension of the transformer                                                           | 256                         \r\n",
      "num_layers      | <class 'int'>   | Number of layers of the transformer                                                           | 2                           \r\n",
      "num_heads       | <class 'int'>   | Number of heads of the transformer                                                            | 8                           \r\n",
      "train_ratio     | <class 'float'> | Ratio of training data to total data                                                          | 0.8                         \r\n",
      "learning_rate   | <class 'float'> | Learning rate of the GAN                                                                      | 0.0001                      \r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "QUICK HELP - These are the special features:\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "General information: \r\n",
      "\r\n",
      "Boolean arguments are given as a single keyword:\r\n",
      "\tSet boolean keyword \"test_keyword\" to True\t->\tpython file.py test_keyword\r\n",
      "Command line arguments are given as a keyword followed by an equal sign and the value:\r\n",
      "\tSet command line argument \"test_keyword\" to \"test_value\"\t->\tpython file.py test_keyword=test_value\r\n",
      "\tWhitespaces are not allowed between a keyword and its value.\r\n",
      "Some keywords can be given list-like: \r\n",
      "\ttest_keyword=test_value1,test_value2\r\n",
      "\tThese keywords are marked with ** in the table.\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "1.\tThe target parameter determines whether you will encode the channels, timeseries, or both (named full):\r\n",
      "\tIf target = channels, then the channels_out parameter will be used\r\n",
      "\tIf target = timeseries, then the timeseries_out parameter will be used\r\n",
      "\tif target = full, then both the channels_out and timeseries_out parameters will be used\r\n",
      "2.\tThe channels_out and timeseries_out parameters indicate the corresponding dimension size output of the encoder\r\n",
      "\t\tFor example, if we havea a 100x30 (timeseries x channel) sample and use timeseries_out=10 & channels_out=4\r\n",
      "\t\twith target=full, our encoder will result in an encoded 10x4 sample\r\n",
      "3.\t\"load_checkpoint\" can be used to load a previously trained autoencoder model and continue training on it.\r\n",
      "\t3.1 If you are loading a previously trained model, it will inherit the following model parameters:\r\n",
      "\t\ttarget, channels_out, timeseries_out. The remainder of the parameters will be used as normal.\r\n",
      "\t3.2 If you do not specify \"path_checkpoint\" the default path is \"trained_ae/checkpoint.pt\"\r\n"
     ]
    }
   ],
   "source": [
    "!python autoencoder_training_main.py help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The help menu above provides a full list of adjustable parameters, but most often only a handfull of these parameters are going to be tweaked, so let's outline these here.\n",
    "\n",
    "`path_dataset`: This is the path to our dataset<br>\n",
    "`save_name`: Path to save the file to (note, this seems not to function on Oscar for some reason)<br><br>\n",
    "\n",
    "`kw_timestep`: The column name prefix of your neural data<br>\n",
    "`channel_label`: The column name of your electrode labels<br><br>\n",
    "\n",
    "`target`: The dimension to encode data, we can encode the timeseries (`time`), the electrodes (`channel`), or both (`full`)<br>\n",
    "`timeseries_out`: The encoded size for electrodes. Only applies if `target` equals `channel` or `full`<br>\n",
    "`channels_out`: The encoded size for electrodes. Only applies if `target` equals `channel` or `full`<br><br>\n",
    "\n",
    "`n_epochs`: The number of epochs to train the autoencoder<br>\n",
    "`ddp`: Whether to activate distributed data parallel training. This means that training will occurs across all GPUs. This does not directly quicken training but makes it more efficient, thus needing less epochs for the same results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder (Sample Data)\n",
    "\n",
    "We will train an autoencoder on the SEEG data momentarily, but for a quick proof of concept let's apply it to our sample data (`ganTrialElectrodeERP_p50_e8_len100.csv`).\n",
    "\n",
    "This data contains `8 electrodes` and `100 datapoints`. The data actually spans 1200ms and was originally recorded at `512Hz` but we have downsampled it to 100 datapoints, which gives it an effect sampling rate of `~83Hz`.\n",
    "\n",
    "Here, we will use the autoencoder to reduce both the electrode and time dimension in half. This means that we will have reduced the data to `1/4` of it's size.<br>\n",
    "`Time`: 100 datapoints -> 50 datapoints<br>\n",
    "`Electrodes`: 8 electrodes -> 4 electrodes<br>\n",
    "\n",
    "We will train for 100 epochs just to quicken the process or else it would take too long to run in this workshop. We will also use ddp training here.<br>\n",
    "\n",
    "Let's define these parameters now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = 'data/ganTrialElectrodeERP_p50_e8_len100.csv'\n",
    "save_name = 'trained_ae/sample_data_autoencoder.pt'\n",
    "\n",
    "kw_timestep = 'Time'\n",
    "channel_label = 'Electrode'\n",
    "\n",
    "target = 'full'\n",
    "timeseries_out = 50\n",
    "channels_out = 4\n",
    "\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our parameters, we can run the training code via the command line interface by starting our command with `!`. Note that the following script has the ddp parameter in it. If you do not have GPUs this will crash. Just delete this parameter and you can train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Command line arguments:\n",
      "-----------------------------------------\n",
      "\n",
      "Distributed training is active\n",
      "Dataset: data/ganTrialElectrodeERP_p50_e8_len100.csv\n",
      "Model save name: trained_ae/sample_data_autoencoder.pt\n",
      "Target: full\n",
      "Keyword for the time step of the dataset: Time\n",
      "Channel label: Electrode\n",
      "Encoded channels size: 4\n",
      "Encoded time series size: 50\n",
      "Number of epochs: 100\n",
      "-----------------------------------------\n",
      "\n",
      "Using device cuda:2.\n",
      "Using device cuda:1.\n",
      "Using device cuda:3.\n",
      "Using device cuda:0.\n",
      "100%|█| 100/100 [03:41<00:00,  2.22s/it, TRAIN LOSS: 0.001625, TEST LOSS: 0.0013\n",
      "Managing checkpoints...\n",
      "100%|█| 100/100 [03:41<00:00,  2.21s/it, TRAIN LOSS: 0.001631, TEST LOSS: 0.0013\n",
      "100%|█| 100/100 [03:41<00:00,  2.21s/it, TRAIN LOSS: 0.001631, TEST LOSS: 0.0013\n",
      "100%|█| 100/100 [03:41<00:00,  2.21s/it, TRAIN LOSS: 0.001636, TEST LOSS: 0.0012\n",
      "GAN training finished.\n",
      "Model states and generated samples saved to file trained_ae/ae_ddp_100ep_20240118_120509.pt.\n"
     ]
    }
   ],
   "source": [
    "!python autoencoder_training_main.py ddp path_dataset={path_dataset} save_name={save_name} target={target} kw_timestep={kw_timestep} channel_label={channel_label} channels_out={channels_out} timeseries_out={timeseries_out} n_epochs={n_epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the new autoencoder saved into the `trained_ae` folder. Again, the `save_name` parameter should have saved the autoencoder with the name specified, but for some reason this does not work on Oscar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir trained_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Autoencoder (Sample Data)\n",
    "\n",
    "Now that the autoencoder is trained, it's important to see if it worked. We can do this by loading data, and running that data through the autoencoder and see how closely the original data and the autoencoder-reconstructed data match. \n",
    "\n",
    "First, let's load the autoencoder and view the configuration of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine paths to data and autoencoder\n",
    "ae_checkpoint = 'trained_ae/ae_ddp_5000ep_20240114_102812.pt'\n",
    "\n",
    "#Load the autoencoder\n",
    "ae_dict = torch.load(ae_checkpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "#Report\n",
    "for key in ae_dict['configuration'].keys():\n",
    "    if key != 'dataloader' and key != 'history':\n",
    "        print(f\"{key}: {ae_dict['configuration'][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the data and see how well the autoencoder can reconstruct the data after reducing it to the given dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and normalize the data to be within [0,1] (as the autoencoder uses normalized data)\n",
    "data_checkpoint = 'data/ganTrialElectrodeERP_p50_e8_len100.csv'\n",
    "dataloader = Dataloader(data_checkpoint, col_label='Condition', channel_label='Electrode')\n",
    "dataset = dataloader.get_data()\n",
    "def norm(data):\n",
    "    return (data-np.min(data)) / (np.max(data) - np.min(data))\n",
    "dataset = norm(dataset.detach().numpy())\n",
    "\n",
    "#Initiate autoencoder\n",
    "autoencoder = initiate_autoencoder(ae_dict, dataset)\n",
    "\n",
    "#Plot 5 random samples to see reconstruction\n",
    "plt.rcParams['figure.figsize'] = [10, 10] #Set figure size\n",
    "fig, axs = plt.subplots(5,1) #Create figure\n",
    "for i in range(5):\n",
    "    sample = np.random.choice(len(dataset), 1) #Randomly determine which sample to plot\n",
    "    data = dataset[sample,1:,:] #Retrieve the sample to plot\n",
    "    axs[i].plot(data[0,:,0], alpha=.5, label='Original') #Plot data\n",
    "    axs[i].plot(autoencoder.decode(autoencoder.encode(torch.from_numpy(data)))[0,:,0].detach().numpy(), alpha=.5, label='Reconstructed') #Plot reconstructed data\n",
    "    axs[i].legend() #Plot legend\n",
    "plt.show() #Show plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
