{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING STARTED: Download the Fully Trained Models\n",
    "\n",
    "We will train our own models in this notebook, but on jupyter notebook it will take too long to train to proper accuracy. As such, we have two fully trained models that you can download at: https://drive.google.com/drive/folders/1oNCkb3baUrRVq9NHZVF1K6af_yfk5BFL?usp=drive_link\n",
    "\n",
    "Download these two models, and place them within the `trained_ae` folder within this repo.\n",
    "\n",
    "# REMINDER: This Notebook Uses GPUs\n",
    "\n",
    "This notebook uses GPUs. There is commented code to switch it to CPUs instead if you like, but for the best performance you want to start an instance with GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Sub-Module of the EEG-GAN Package\n",
    "\n",
    "The autoencdoer code we are using today is part of our larger package, [EEG-GAN](https://autoresearch.github.io/EEG-GAN/). This package uses generative modelling - specifically, generative adversarial networks - to increase sample sizes by creating synthetic participants. We have shown that doing this enhances classification performance by providing more training samples for classification models. You can read more about this in our [published manuscript](https://escholarship.org/uc/item/9gz8g908). This package may actually be useful here with this data for this challenge; however, it has not been tested using SEEG data nor with so many conditions (i.e., up to 30 video clips) and with so little samples, so we opted not to present this package here. \n",
    "\n",
    "With that said, we are actively developing EEG-GAN v2.0, and this future version will contain an embedded autoencoder, which is the code included within this repo. Although this package is in active development, the autoencoder component is stabilized and thoroughly tested to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as sst\n",
    "\n",
    "import torch\n",
    "from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present\n",
    "from nn_architecture.ae_networks import TransformerAutoencoder, TransformerDoubleAutoencoder, TransformerFlattenAutoencoder\n",
    "from helpers.dataloader import Dataloader\n",
    "\n",
    "pd.set_option('display.max_columns', 5)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_data(samples, fs=1024, notch_freq=60.0, quality_factor=20.0, data_prefix='Time'):\n",
    "    \n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    b_notch, a_notch = ss.iirnotch(notch_freq, quality_factor, fs)\n",
    "    filtered_signal = ss.filtfilt(b_notch, a_notch, samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    samples.iloc[:, neural_indices[0]:] = filtered_signal\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def filter_data(samples, lowcut=0.1, highcut=200, fs=1024, order=5, padding=3, data_prefix='Time'):\n",
    "    '''\n",
    "    ...add here\n",
    "    '''\n",
    "    padding_datapoints = padding*fs\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = ss.butter(order, [low, high], btype='bandpass')\n",
    "    \n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Filter signal\n",
    "    filtered_signal = ss.filtfilt(b, a, samples.iloc[:, neural_indices[0]:], padlen=(padding*fs), axis=1)\n",
    "    samples.iloc[:, neural_indices[0]:] = filtered_signal\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def artifact_rejection(samples, threshold=5, data_prefix='Time'):\n",
    "\n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Determine gradients and z-scores\n",
    "    grad = np.gradient(samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    g_zs = sst.zscore(grad, axis=1)\n",
    "    h_zs = sst.zscore(samples.iloc[:, neural_indices[0]:], axis=1)\n",
    "    \n",
    "    #Determine which segments should be removed\n",
    "    reject = np.logical_or(np.abs(g_zs) > threshold, np.abs(h_zs) > threshold) #Determine all datapoints that violate criteria\n",
    "    reject = np.any(reject, axis=1)\n",
    "    \n",
    "    #Convert to which segments should be kept\n",
    "    keep = [not r for r in reject]\n",
    "    \n",
    "    #Report\n",
    "    print(f\"{np.sum(reject)} of {len(samples)} ({np.round((np.sum(reject)/len(samples))*100)}%) of segments were rejected.\")\n",
    "    \n",
    "    return samples.loc[keep]\n",
    "\n",
    "def downsample_data(samples, sampling_rate=1024, downsampled_rate=256, data_prefix='Time'):\n",
    "\n",
    "    #Determine columns containing neural data\n",
    "    neural_indices = [column_index for column_index, column_name in enumerate(samples.columns) if data_prefix in column_name]\n",
    "\n",
    "    #Determine length of samples in seconds\n",
    "    number_of_seconds = samples.iloc[0][neural_indices[0]:].shape[0]/sampling_rate\n",
    "    \n",
    "    #Determine number of datapoints after downsampling\n",
    "    new_number_of_datapoints = int(number_of_seconds*downsampled_rate)\n",
    "    \n",
    "    #Determine new column names\n",
    "    sr_ratio = sampling_rate/downsampled_rate\n",
    "    metadata_names = samples.columns[:neural_indices[0]]\n",
    "    downsampled_time_names = [column_name for column_index, column_name in enumerate(samples.columns[neural_indices[0]:]) if column_index%sr_ratio == 0]\n",
    "    downsampled_names = list(metadata_names) + list(downsampled_time_names)\n",
    "    \n",
    "    #Iterate through samples to downsample\n",
    "    downsampled_samples = pd.DataFrame(columns=downsampled_names)\n",
    "    for sample_index in range(len(samples)):\n",
    "        current_sample = samples.iloc[sample_index]\n",
    "        current_metadata = current_sample.iloc[:neural_indices[0]]\n",
    "        current_sample = current_sample.iloc[neural_indices[0]:]\n",
    "        downsampled_sample = scipy.signal.resample(current_sample, new_number_of_datapoints)\n",
    "        new_sample = pd.DataFrame([list(current_metadata) + list(downsampled_sample)], columns=downsampled_names)\n",
    "        downsampled_samples = pd.concat([downsampled_samples, new_sample])\n",
    "\n",
    "    return downsampled_samples       \n",
    "\n",
    "def initiate_autoencoder(ae_dict, dataset):\n",
    "\n",
    "    n_channels = dataset.shape[-1]\n",
    "    sequence_length = dataset.shape[1] - 1\n",
    "\n",
    "    if ae_dict['configuration']['target'] == 'channels':\n",
    "        autoencoder = TransformerAutoencoder(input_dim=n_channels,\n",
    "                                       output_dim=ae_dict['configuration']['channels_out'],\n",
    "                                       output_dim_2=sequence_length,\n",
    "                                       target=TransformerAutoencoder.TARGET_CHANNELS,\n",
    "                                       hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                       num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                       num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    elif ae_dict['configuration']['target'] == 'time':\n",
    "        autoencoder = TransformerAutoencoder(input_dim=sequence_length,\n",
    "                                       output_dim=ae_dict['configuration']['timeseries_out'],\n",
    "                                       output_dim_2=n_channels,\n",
    "                                       target=TransformerAutoencoder.TARGET_TIMESERIES,\n",
    "                                       hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                       num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                       num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    elif ae_dict['configuration']['target'] == 'full':\n",
    "        autoencoder = TransformerDoubleAutoencoder(input_dim=n_channels,\n",
    "                                             output_dim=ae_dict['configuration']['output_dim'],\n",
    "                                             output_dim_2=ae_dict['configuration']['output_dim_2'],\n",
    "                                             sequence_length=sequence_length,\n",
    "                                             hidden_dim=ae_dict['configuration']['hidden_dim'],\n",
    "                                             num_layers=ae_dict['configuration']['num_layers'],\n",
    "                                             num_heads=ae_dict['configuration']['num_heads'],).to('cpu')\n",
    "    else:\n",
    "        raise ValueError(f\"Encode target '{ae_dict['configuration']['target']}' not recognized, options are 'channels', 'time', or 'full'.\")\n",
    "    consume_prefix_in_state_dict_if_present(ae_dict['model'],'module.')\n",
    "    autoencoder.load_state_dict(ae_dict['model'])\n",
    "    autoencoder.device = torch.device('cpu')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Re-Organizing the Data\n",
    "\n",
    "The autoencoder needs a very specific data structure to work. The csv files provided for each participant is already in this data stucture, so we will not have to do too much data manipulation in this workshop, which is great.\n",
    "\n",
    "We use a very simple 2-dimensional data structure where each row is a different trial-level sample for one condition and one electrode. We will direct the autoencoder with column names to guide it in understanding what is metadata and what is neural data. For example, below we will inform the autoencoder that the electrode labels are within the `Electrode` column (i.e., `channel_label=Electrode`) and that the neural data begins with the prefix `Time` (i.e., `kw_timestep=Time`). Behind the scenes, the autoencoder will use the electrode labels to transform the data from a 2D matrix to a 3D matrix and will only include data with the given prefix as the neural data. The autoencoder will ignore any other columns not specified. For example, when looking at the data snippet below, it will ignore the columns `ParticipantID`, `Phase`, and `Condition` because it does not need this information.\n",
    "\n",
    "| participant_id | Phase | Condition | Electrode | Time0000 | Time0001 | Time0002 | Time0003 | ... |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| e0010GP | Encoding | 1 | HIP1 | 2.28 | -4.44 | -0.98 | -5.67 | ... |\n",
    "| e0010GP | Encoding | 1 | HIP2 | 11.67 | 0.66 | 1.43 | 11.62 | ... |\n",
    "| e0010GP | Encoding | 2 | HIP1 | 11.90 | 8.67 | 1.85 | 0.73 | ... |\n",
    "| e0010GP | Encoding | 2 | HIP2 | 6.73 | 3.63 | 3.80 | 5.63 | ... |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "In this workshop, we will be looking through participant data multiple times. Instead of re-loading each dataset over and over again, we might as well load them all once and keep them stored in a variable. We will do this by creating a dictionary where each key is a different participant's dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we designate where the data is\n",
    "data_path = os.path.abspath('/oscar/data/brainstorm-ws/seeg_data/Memory Task Data/Epilepsy/Monitoring/')\n",
    "\n",
    "#We will also manually set which participants to include in the autoencoder training\n",
    "participant_files = [\n",
    "              f'{data_path}/e0019VQ_preprocessed_data.csv',\n",
    "              f'{data_path}/e0011XQ_preprocessed_data.csv',\n",
    "              f'{data_path}/e0022ZG_preprocessed_data.csv',\n",
    "              f'{data_path}/e0015TJ_preprocessed_data.csv',\n",
    "              f'{data_path}/e0014VG_preprocessed_data.csv',\n",
    "              f'{data_path}/e0010GP_preprocessed_data.csv',\n",
    "              f'{data_path}/e0020JA_preprocessed_data.csv',\n",
    "              f'{data_path}/e0024DV_preprocessed_data.csv',\n",
    "              f'{data_path}/e0013LW_preprocessed_data.csv',\n",
    "              #f'{data_path}/e0017MC_preprocessed_data.csv', #Does not have any electrodes with the HIP naming convention\n",
    "              #f'{data_path}/e0016YR_preprocessed_data.csv', #Does not have any electrodes with the HIP naming convention\n",
    "              ]\n",
    "\n",
    "#We will determine all participant ids \n",
    "participant_ids = [participant_file.split('/')[-1].replace('_data.csv','') for participant_file in participant_files]\n",
    "\n",
    "#Now, let's load all participant data and save them into a dictionary\n",
    "participant_data = {} #Create empty dictionary\n",
    "for participant_index, participant_file in enumerate(tqdm(participant_files)):\n",
    "    participant_data[participant_ids[participant_index]] = pd.read_csv(participant_file, dtype = {'Electrode': str})\n",
    "                                                                       \n",
    "#We can report what we created\n",
    "print(f\"Dictionary Keys: {participant_data.keys()}\\n\")\n",
    "print('Example dataset:')\n",
    "print(participant_data[list(participant_data.keys())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrodes\n",
    "One other requirement that the autoencoder has is that every participant has the same number of electrodes. This toolbox was designed for EEG analyses, and this is the standard practice in that research---but as we have seen throughout this week, our participants have both a different number of electrodes and electrodes in different locations. What this means is that we are going to have to make some decisions on how many electrodes, and which electrodes, to include for each participant.\n",
    "\n",
    "Later on, we are going to use an adapted version of Younes Strittmatter's classification notebook, which focuses on hippocampus electrodes (i.e., uses electrodes with the `HIP` acronym), so here we will only focus on these electrodes. Let's cycle through each participant and see which hippocampus electrodes they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the brain region of interest\n",
    "brain_region = 'HIP'\n",
    "\n",
    "#Iterate through all participants\n",
    "for participant_id in participant_data.keys():\n",
    "    print(f\"Participant: {participant_id}\") #Report which participant\n",
    "    electrodes = [electrode.split('_')[-1] for electrode in participant_data[participant_id]['Electrode'].unique() if brain_region.lower() in electrode.lower()] #Determine corresponding electrodes\n",
    "    print(f\"Electrodes ({len(electrodes)}):\\n{electrodes}\\n\") #Report found electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the participants have quite different electrodes within the hippocamps. This could mean that participants have electrodes in different locations but in some cases it could also simply indicate that there are different naming conventions per participant. Ideally, we would be able to find a number of electrodes that are completely consistent across participants but this may not be possible with the current dataset due to the nature of collecting SEEG data. \n",
    "\n",
    "So, instead of selecting common electrodes, we will randomly select a set of electrodes for each participant and hope that they are similar enough to consider consistent across participants. The classification code that Younes presented has us averaging across these electrodes anyhow, so this seems like a safe enough process here. \n",
    "\n",
    "The participants also have a different number of electrodes within the hippocampus. As the autoencoder needs the same number of electrodes for each participant, we need to select a number of electrodes equal to or less than the lower end of this range.\n",
    "\n",
    "You could select which electrodes to include manually if you would like, but instead here we will randomly select hippocampus electrodes. We will set a seed in our `numpy` random function first so that we end up with the same electrodes every time we run this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set numpy seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#Determine the brain region of interest\n",
    "brain_region = 'HIP'\n",
    "number_of_electrodes = 20\n",
    "\n",
    "#Iterate through each participant and collect 12 electrodes per participant \n",
    "selected_electrodes = {} #Setup a variable to collect the selected electrodes\n",
    "for participant_id in participant_data.keys():\n",
    "    electrodes = [electrode for electrode in participant_data[participant_id]['Electrode'].unique() if brain_region.lower() in electrode.lower()] #Determine corresponding electrodes\n",
    "    if electrodes:\n",
    "        electrodes = np.random.choice(electrodes, number_of_electrodes, replace=False) #Randomly select a number of electrodes equal to the variable number_of_electrodes\n",
    "    selected_electrodes[participant_id] = electrodes #Save selected electrodes as a list of lists to use later\n",
    "    \n",
    "#Print as dataframe\n",
    "pd.set_option('display.max_columns', len(participant_files)) #Show more columns just for a moment\n",
    "print(pd.DataFrame(np.array([selected_electrodes[key] for key in selected_electrodes.keys()]).T, columns = [participant_file.split('/')[-1].replace('_data.csv','') for participant_file in participant_files]))\n",
    "pd.set_option('display.max_columns', 5) #Reset to showing 5 columns max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data Files\n",
    "\n",
    "Another requirement the autoencoder has is that all of the data is within the same csv file. Above we have created a list of participants to include and we have determined which electrodes to use per participant, so now we will extract the participant data for each participant and add it all to a single dataframe and then save this dataframe in our current repo's `data` sub-directory. \n",
    "\n",
    "The autoencoder should only be trained with the data that you will use to train your classification model. In the classification code, we will train our model on the `Encoding` and `SameDayRecall` phases and validate it's performance on the `NextDayRecall` phase.\n",
    "\n",
    "Furthermore, the autoencoder model can get quite big with large datasets, and having 5 seconds at 1024Hz (i.e., 5120 datapoints) is quite large. Large data and models may fail if they fill your memory. So, we will here condense by downsampling the data using the `downsample_data` function. In this workshop we will reduce the sampling rate from 1024Hz to 128Hz.\n",
    "\n",
    "WARNING: Sharp wave ripples (SWR) are in the 80 - 100Hz range and the frequencies you can extract from data equal half of the sampling rate (this is called the Nyquist Limit). So, if we downsample to 128Hz, we can only extract frequencies up to 64Hz, cutting out all of these SWR frequencies. If we want to keep these SWR frequencies, it would be better to downsample to 256Hz, which would allow us to extract frequencies up to 128Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_files(participant_data, selected_electrodes, phases=['Encoding', 'SameDayRecall'], conditions = [1, 2], save_name='data/seeg_ae_training_data.csv'):\n",
    "    #We will first create a dataframe to store all of our participant data in\n",
    "    times = [f\"Time{str(datapoint).zfill(4)}\" for datapoint in range(5120)]\n",
    "    ae_data = pd.DataFrame(columns=['Participant_ID','Phase','Condition','Electrode','Error_Position','Error_Color']+times)\n",
    "    \n",
    "    #Iterate through each participant, filtering the data to the specified phases and electrodes, and store it into the master variable\n",
    "    for participant_id in tqdm(participant_data.keys()):\n",
    "        phase_indices = participant_data[participant_id]['Phase'].isin(phases) #Phase indices\n",
    "        electrode_indices = participant_data[participant_id]['Electrode'].isin(selected_electrodes[participant_id]) #Electrode indices\n",
    "        conditions_indices = participant_data[participant_id]['Condition'].isin(conditions)\n",
    "        p_data = participant_data[participant_id][(phase_indices) & (electrode_indices) & (conditions_indices)] #Extract participant data\n",
    "        \n",
    "        #We also need to rename all of the electrodes so that there is consistency of electrode labels across participants\n",
    "        for electrode_index, selected_electrode in enumerate(selected_electrodes[participant_id]):\n",
    "            p_data = p_data.replace({selected_electrode: electrode_index+1})\n",
    "\n",
    "        for phase_index, selected_phase in enumerate(phases):\n",
    "            p_data = p_data.replace({selected_phase: phase_index+1})\n",
    "            \n",
    "        #Pre-Process Data\n",
    "        if p_data.shape[0] > 0: #Check to see if data contains variables of interest\n",
    "            #p_data = notch_data(p_data, fs=1024, notch_freq=60.0, quality_factor=20.0, data_prefix='Time')\n",
    "            #p_data = filter_data(p_data, lowcut=0.1, highcut=120, fs=1024, order=5, padding=3, data_prefix='Time') #Note: You should really only filter the continuous data, and not segmented data like we are doing here because the latter will introduct 'edge artifacts'\n",
    "            #p_data = artifact_rejection(p_data, threshold=10, data_prefix='Time') #Note: Artifact rejection might be an issue here, as it can cause some electrodes to have no data. The autoencoder requires all specified electrodes to have data. So, you might rather introduce artifact rejection after reconstructing the data via the autoencoder.\n",
    "            p_data = downsample_data(p_data, sampling_rate=1024, downsampled_rate=128, data_prefix='Time')\n",
    "\n",
    "            #Add data to the master variable\n",
    "            ae_data = pd.concat([ae_data, p_data])\n",
    "\n",
    "    #Sort data\n",
    "    ae_data = ae_data.sort_values(by=['Phase', 'Condition', 'Participant_ID', 'Electrode'])\n",
    "    \n",
    "    #Save data to csv\n",
    "    ae_data = ae_data.dropna(how=\"all\", axis=1) #Removes columns that are empty due to downsampling\n",
    "    ae_data.to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating training data...')\n",
    "create_data_files(participant_data, selected_electrodes, phases=['Encoding', 'SameDayRecall'], conditions = [1,2], save_name='data/seeg_ae_training_data.csv')\n",
    "\n",
    "print('Creating validation data...')\n",
    "create_data_files(participant_data, selected_electrodes, phases=['NextDayRecall'], conditions = [1,2], save_name='data/seeg_ae_validation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the file is now within our directory. As this file is now ready and in our directory, we are ready to train the autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "The autoencoder is designed to be used within the command line interface (CLI). What this means is that we do not use functions, such as `train_autoencoder(training_data)` as is common with Python. Insteead we are going to run commands such as `python autoencoder_training_main.py ...` where `...` are our input parameters. If you are not familiar with CLI programming, not to worry as we will show you how to do so here. \n",
    "\n",
    "In jupyter notebooks, if you begin a line of code with `!` it will run via the CLI. Our Autoencoder has a `help` parameter so let's see how that works here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python autoencoder_training_main.py help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The help menu above provides a full list of adjustable parameters, but most often only a handfull of these parameters are going to be tweaked, so let's outline these here.\n",
    "\n",
    "`path_dataset`: This is the path to our dataset<br>\n",
    "`save_name`: Path to save the file to (note, this seems not to function on Oscar for some reason)<br><br>\n",
    "\n",
    "`kw_timestep`: The column name prefix of your neural data<br>\n",
    "`channel_label`: The column name of your electrode labels<br><br>\n",
    "\n",
    "`target`: The dimension to encode data, we can encode the timeseries (`time`), the electrodes (`channel`), or both (`full`)<br>\n",
    "`timeseries_out`: The encoded size for electrodes. Only applies if `target` equals `channel` or `full`<br>\n",
    "`channels_out`: The encoded size for electrodes. Only applies if `target` equals `channel` or `full`<br><br>\n",
    "\n",
    "`n_epochs`: The number of epochs to train the autoencoder<br>\n",
    "`ddp`: Whether to activate distributed data parallel training. This means that training will occurs across all GPUs. This does not directly quicken training but makes it more efficient, thus needing less epochs for the same results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder (Sample Data)\n",
    "\n",
    "We will train an autoencoder on the SEEG data momentarily, but for a quick proof of concept let's apply it to our sample data (`ganTrialElectrodeERP_p50_e8_len100.csv`).\n",
    "\n",
    "This data contains `8 electrodes` and `100 datapoints`. The data actually spans 1200ms and was originally recorded at `512Hz` but we have downsampled it to 100 datapoints, which gives it an effect sampling rate of `~83Hz`.\n",
    "\n",
    "Here, we will use the autoencoder to reduce both the electrode and time dimension in half. This means that we will have reduced the data to `1/4` of it's size.<br>\n",
    "`Time`: 100 datapoints -> 50 datapoints<br>\n",
    "`Electrodes`: 8 electrodes -> 4 electrodes<br>\n",
    "\n",
    "We will train for 10 epochs just to quicken the process or else it would take too long to run in this workshop. We will also use ddp training here.<br>\n",
    "\n",
    "Let's define these parameters now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = 'data/ganTrialElectrodeERP_p50_e8_len100.csv'\n",
    "save_name = 'trained_ae/sample_data_autoencoder.pt'\n",
    "\n",
    "kw_timestep = 'Time'\n",
    "channel_label = 'Electrode'\n",
    "\n",
    "target = 'full'\n",
    "timeseries_out = 50\n",
    "channels_out = 4\n",
    "\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our parameters, we can run the training code via the command line interface by starting our command with `!`. Note that the following script has the ddp parameter in it. If you do not have GPUs this will crash. Just delete this parameter and you can train on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GPU:\n",
    "!python autoencoder_training_main.py ddp path_dataset={path_dataset} save_name={save_name} target={target} kw_timestep={kw_timestep} channel_label={channel_label} channels_out={channels_out} timeseries_out={timeseries_out} n_epochs={n_epochs}\n",
    "\n",
    "#Using CPU\n",
    "#!python autoencoder_training_main.py path_dataset={path_dataset} save_name={save_name} target={target} kw_timestep={kw_timestep} channel_label={channel_label} channels_out={channels_out} timeseries_out={timeseries_out} n_epochs={n_epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you will see what the autoencoder was saved as and you can see the new autoencoder saved into the `trained_ae` folder. Again, the `save_name` parameter should have saved the autoencoder with the name specified, but for some reason this does not work on Oscar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir trained_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Autoencoder (Sample Data)\n",
    "\n",
    "Now that the autoencoder is trained, it's important to see if it worked. We can do this by loading data, and running that data through the autoencoder and see how closely the original data and the autoencoder-reconstructed data match. \n",
    "\n",
    "First, let's load the autoencoder and view the configuration of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine paths to data and autoencoder\n",
    "#ae_checkpoint = 'trained_ae/ae_ddp_500ep_20240118_123640.pt'\n",
    "ae_checkpoint = 'trained_ae/fully_trained_sample_data_autoencoder.pt'\n",
    "\n",
    "#Load the autoencoder\n",
    "ae_dict = torch.load(ae_checkpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "#Report\n",
    "for key in ae_dict['configuration'].keys():\n",
    "    if key != 'dataloader' and key != 'history':\n",
    "        print(f\"{key}: {ae_dict['configuration'][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the data and see how well the autoencoder can reconstruct the data after reducing it to the given dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and normalize the data to be within [0,1] (as the autoencoder uses normalized data)\n",
    "data_checkpoint = 'data/ganTrialElectrodeERP_p50_e8_len100.csv'\n",
    "dataloader = Dataloader(data_checkpoint, col_label='Condition', channel_label='Electrode')\n",
    "dataset = dataloader.get_data()\n",
    "def norm(data):\n",
    "    return (data-np.min(data)) / (np.max(data) - np.min(data))\n",
    "dataset = norm(dataset.detach().numpy())\n",
    "\n",
    "#Initiate autoencoder\n",
    "autoencoder = initiate_autoencoder(ae_dict, dataset)\n",
    "\n",
    "#Plot 5 random samples to see reconstruction\n",
    "plt.rcParams['figure.figsize'] = [10, 10] #Set figure size\n",
    "fig, axs = plt.subplots(5,1) #Create figure\n",
    "for i in range(5):\n",
    "    sample = np.random.choice(len(dataset), 1) #Randomly determine which sample to plot\n",
    "    data = dataset[sample,1:,:] #Retrieve the sample to plot\n",
    "    axs[i].plot(data[0,:,0], alpha=.5, label='Original') #Plot data\n",
    "    axs[i].plot(autoencoder.decode(autoencoder.encode(torch.from_numpy(data)))[0,:,0].detach().numpy(), alpha=.5, label='Reconstructed') #Plot reconstructed data\n",
    "    axs[i].legend() #Plot legend\n",
    "plt.show() #Show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder (SEEG Data)\n",
    "\n",
    "Now let's train the autoencoder on the SEEG data that we created at the beginning of this workshop (`seeg_ae_training_data.csv`).\n",
    "\n",
    "This data contains the `12 electrodes` that we chose. Furthermore, we downsampled the data from `1024Hz` to `128Hz`, and contains 5 seconds of data, so we have `128*5 = 640` datapoints.\n",
    "\n",
    "Here, we will use the autoencoder to reduce both the electrode and time dimension in half. This means that we will have reduced the data to `1/4` of it's size.<br>\n",
    "`Time`: 640 datapoints -> 320 datapoints<br>\n",
    "`Electrodes`: 20 electrodes -> 10 electrodes<br>\n",
    "\n",
    "We will train for 1000 epochs just to quicken the process or else it would take too long to run in this workshop. We will also use ddp training here.<br>\n",
    "\n",
    "Let's define these parameters now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = 'data/seeg_ae_training_data.csv'\n",
    "save_name = 'trained_ae/seeg_autoencoder.pt'\n",
    "\n",
    "kw_timestep = 'Time'\n",
    "channel_label = 'Electrode'\n",
    "\n",
    "target = 'full'\n",
    "timeseries_out = 320\n",
    "channels_out = 10\n",
    "\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GPU:\n",
    "!python autoencoder_training_main.py ddp path_dataset={path_dataset} save_name={save_name} target={target} kw_timestep={kw_timestep} channel_label={channel_label} channels_out={channels_out} timeseries_out={timeseries_out} n_epochs={n_epochs} batch_size=16\n",
    "\n",
    "#Using CPU:\n",
    "#!python autoencoder_training_main.py path_dataset={path_dataset} save_name={save_name} target={target} kw_timestep={kw_timestep} channel_label={channel_label} channels_out={channels_out} timeseries_out={timeseries_out} n_epochs={n_epochs} batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Autoencoder (SEEG Data)\n",
    "\n",
    "Now that the autoencoder is trained, it's important to see if it worked. We can do this by loading data, and running that data through the autoencoder and see how closely the original data and the autoencoder-reconstructed data match. \n",
    "\n",
    "First, let's load the autoencoder and view the configuration of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine paths to data and autoencoder\n",
    "#ae_checkpoint = 'trained_ae/ae_ddp_5000ep_20240119_094157.pt' #<------Your trained model save name may look something like this\n",
    "ae_checkpoint = 'trained_ae/fully_trained_seeg_autoencoder.pt'\n",
    "\n",
    "#Load the autoencoder\n",
    "ae_dict = torch.load(ae_checkpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "#Report\n",
    "for key in ae_dict['configuration'].keys():\n",
    "    if key != 'dataloader' and key != 'history':\n",
    "        print(f\"{key}: {ae_dict['configuration'][key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the data and see how well the autoencoder can reconstruct the data after reducing it to the given dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and normalize the data to be within [0,1] (as the autoencoder uses normalized data)\n",
    "data_checkpoint = 'data/seeg_ae_training_data.csv'\n",
    "dataloader = Dataloader(data_checkpoint, col_label='Condition', channel_label='Electrode')\n",
    "dataset = dataloader.get_data()\n",
    "def norm(data):\n",
    "    return (data-np.min(data)) / (np.max(data) - np.min(data))\n",
    "dataset = norm(dataset.detach().numpy())\n",
    "\n",
    "#Initiate autoencoder\n",
    "autoencoder = initiate_autoencoder(ae_dict, dataset)\n",
    "\n",
    "#Plot 5 random samples to see reconstruction\n",
    "plt.rcParams['figure.figsize'] = [10, 10] #Set figure size\n",
    "fig, axs = plt.subplots(5,1) #Create figure\n",
    "for i in range(5):\n",
    "    sample = np.random.choice(len(dataset), 1) #Randomly determine which sample to plot\n",
    "    data = dataset[sample,1:,:] #Retrieve the sample to plot\n",
    "    axs[i].plot(data[0,:,0], alpha=.5, label='Original') #Plot data\n",
    "    axs[i].plot(autoencoder.decode(autoencoder.encode(torch.from_numpy(data)))[0,:,0].detach().numpy(), alpha=.5, label='Reconstructed') #Plot reconstructed data\n",
    "    axs[i].legend() #Plot legend\n",
    "plt.show() #Show plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
